{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for image classification\n",
    "\n",
    "1.A 2D convolutional layer to the model with 100 filters, a filter size of 3x3, and a rectified linear unit (ReLU) activation function. The input_shape argument specifies the shape of the input image, which is 150x150 pixels with RGB color channel.\n",
    "\n",
    "2.A max pooling layer to the model with a pool size of 2x2. This layer reduces the spatial size of the output from the previous convolutional layer by taking the maximum value within each 2x2 window.\n",
    "\n",
    "3.A flattening layer to the model, which converts the output from the previous layer into a 1D vector that can be fed into a fully connected layer.\n",
    "\n",
    "4.A dropout layer to the model, which randomly drops out 50% of the connections between the previous layer and the next layer during training.\n",
    "\n",
    "5.Dense layer-a fully connected layer to the model with 50 neurons and a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 100)     2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6480050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,573,052\n",
      "Trainable params: 6,573,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),        # prevent overfitting by reducing the model's reliance on any one feature\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax') # Used in multiclass classification problems to convert the output of the model into a probability distribution over the classes.\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model Optimizer, Loss and Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', # minimize this difference\n",
    "              metrics=['acc']) # measures the percentage of correct predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an image data generator for training the model to  use various image augmentation techniques, such as rotation, shifting, and flipping, which are applied to the images during training to increase the variety of data the model is to be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3268 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"F:\\dataset/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, #number of samples in each batch\n",
    "                                                    target_size=(150, 150)) #resizes the images to 150x150 pixels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an image data generator for validation data to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 818 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_DIR = \"F:\\dataset/test\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255) #image transformation,Normalizing by rescaling the pixel values to be between 0 and 1\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 26/327 [=>............................] - ETA: 3:17 - loss: 0.9379 - acc: 0.5077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\cvpr_zidan\\lib\\site-packages\\PIL\\Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - ETA: 0s - loss: 0.5740 - acc: 0.7041INFO:tensorflow:Assets written to: model2-001.model\\assets\n",
      "327/327 [==============================] - 237s 724ms/step - loss: 0.5740 - acc: 0.7041 - val_loss: 0.4597 - val_acc: 0.8007\n",
      "Epoch 2/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.4284 - acc: 0.8296INFO:tensorflow:Assets written to: model2-002.model\\assets\n",
      "327/327 [==============================] - 220s 674ms/step - loss: 0.4284 - acc: 0.8296 - val_loss: 0.2867 - val_acc: 0.9010\n",
      "Epoch 3/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.4048 - acc: 0.8351INFO:tensorflow:Assets written to: model2-003.model\\assets\n",
      "327/327 [==============================] - 232s 709ms/step - loss: 0.4048 - acc: 0.8351 - val_loss: 0.2801 - val_acc: 0.8998\n",
      "Epoch 4/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3748 - acc: 0.8602INFO:tensorflow:Assets written to: model2-004.model\\assets\n",
      "327/327 [==============================] - 229s 699ms/step - loss: 0.3748 - acc: 0.8602 - val_loss: 0.2598 - val_acc: 0.9193\n",
      "Epoch 5/10\n",
      "327/327 [==============================] - 215s 658ms/step - loss: 0.3832 - acc: 0.8479 - val_loss: 0.2940 - val_acc: 0.8888\n",
      "Epoch 6/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3607 - acc: 0.8602INFO:tensorflow:Assets written to: model2-006.model\\assets\n",
      "327/327 [==============================] - 204s 625ms/step - loss: 0.3607 - acc: 0.8602 - val_loss: 0.2496 - val_acc: 0.9083\n",
      "Epoch 7/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3472 - acc: 0.8589INFO:tensorflow:Assets written to: model2-007.model\\assets\n",
      "327/327 [==============================] - 203s 622ms/step - loss: 0.3472 - acc: 0.8589 - val_loss: 0.2427 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "327/327 [==============================] - 201s 615ms/step - loss: 0.3265 - acc: 0.8770 - val_loss: 0.2728 - val_acc: 0.8863\n",
      "Epoch 9/10\n",
      "327/327 [==============================] - 204s 624ms/step - loss: 0.3327 - acc: 0.8712 - val_loss: 0.2484 - val_acc: 0.9120\n",
      "Epoch 10/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3027 - acc: 0.8785INFO:tensorflow:Assets written to: model2-010.model\\assets\n",
      "327/327 [==============================] - 206s 629ms/step - loss: 0.3027 - acc: 0.8785 - val_loss: 0.2377 - val_acc: 0.9218\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint]) # After each epoch,the ModelCheckpoint callback will save the model if its validation loss is lower than the previous best. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Zidan_Face_mask_detection_model_2.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking live input through camera to detect face mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "#  path to the model file\n",
    "model_path = \"./Zidan_Face_mask_detection_model_2.h5\"\n",
    "\n",
    "# Load the model \n",
    "model = load_model(model_path)\n",
    "\n",
    "labels_dict={0:'No Mask',1:'Mask on'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file,  pre-trained classifier used by OpenCV to detect faces.\n",
    "classifier = cv2.CascadeClassifier('F:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('Mask_Detection',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if q key is press then break out of the loop \n",
    "    if key == 113: #The q key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr_zidan tf2.4 py3.8",
   "language": "python",
   "name": "cvpr_zidan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
